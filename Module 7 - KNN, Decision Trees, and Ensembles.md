
# Review
- Supervised Learning
	- Input
	- Output
	- Function
	- Data
	- ![[Pasted image 20240215113848.png]]
- Predictive Generalization
	- Training Data (x, y)
	- Test Data
	- Generalization - Ability to apply the model to data outside of the training model
	- Overfitting
	- Red Dots: Test/New data, black line: linear regression model, blue line: overfit model ![[Pasted image 20240215113932.png]]
- Optimization by Gradient Descent
	- Model
	- Parameters [w0, w1]
	- Loss - Difference between the predicted values SUM(y^ - y)^2
	- Objective - Choose the parameters that reduce loss
	- ![[Pasted image 20240215114231.png]]
- Adding Complexity with Layers
	- ![[Pasted image 20240215115936.png]]
	- Adding intermediate variables, h1 and h2
	- y = b3 + w1h1 + w2h2
		- h1 = b1 + w01x1 + w02x2
	- Parameters: 9 - all the things in blue
	- Loss = yhat - y
	- Objective, the same, but more complicated because propagating loss back
- Learning Non-Linear Transformations
	- ![[Pasted image 20240215121054.png]]
- Experimental Process
	- Baseline
	- Add complexity gradually
	- Analyze errors
	- Iterate on a model or data
	- ![[Pasted image 20240215121317.png]]
	- If you over train the data, the test error rate will go up meaning that the data is overfit ![[Pasted image 20240215121514.png]]

# Nearest Neighbors
- Parametric v Non-parametric
	- Parametric: Model has learned parameters
	- Non-parametric: No learned Parameters
		- Memorizes the training data and predicts only based on what it has memorized
- Nearest Neighbor
	- Training: no training!
	- Inference
		- Find the closest training example
		- Return its label
	- ![[Pasted image 20240215123912.png]]
- Calculating Distances
	- ![[Pasted image 20240215124156.png]]
- Norm of A
	- ![[Pasted image 20240215124243.png]]
- Dot Products
	- The larger the dot product the MORE similar they are
	- ![[Pasted image 20240215140020.png]]
	- Min similarity: -1
	- Max similarity: 1
- K-nearest neighbor
	- Training: no training!
	- Inference
		- Find the closest k training examples
		- Return an average over the k labels
			- May be weighted average
	- Way of smoothing out nearest neighbor
	- Commonly compute distances using dot products
- Decision Boundaries
	- KNN produces complex decision boundaries
	- Increasing K smooths the boundary
	- ![[Pasted image 20240215151430.png]]
- Occam's Razor
	- Principle of Parsimony
		- Prefer the simplest explanation (fewest assumptions)
		- Only add complexity as needed
	- ![[Pasted image 20240215153420.png]] ![[Pasted image 20240215153456.png]]

- KNN vs FFNN Classifiers
	- KNN Approach
		- Use the input vector space
		- Many examples of each class
		- Apply the label from the closest example
	- Neural Network Approach
		- The network contorts the input vector space into a new vector space
		- A single example of each class
		- Apply the label from the closest "example"

# Decision Trees
- Baseline/ combined with Neural Networks
- KNN Pros and Cons
	- Pro 
		- Predictions are easy to understand (interpretable)
		- Might work with very few labeled examples
	- Con
		- Relies on a distance metric
		- Easily fooled by outliers (noisy training examples)
		- Poor scaling (slow)
		- Poor generalization
- Good case for a decision tree:
	- ![[Pasted image 20240215160608.png]]
	- ![[Pasted image 20240215160630.png]]
		- Internal Nodes: Test the value of a feature
		- Leaf nodes: Output a predicted class or value
	- Expressiveness
		- Express any logical function of the features
		- Equivalent to a truth table for binary variables
		- ![[Pasted image 20240215193100.png]]
	- Decision Boundaries
		- Boundaries are rectangular partitions
		- Approximate diagonal lines with steps
			- Won't be a true diagonal, will just be a bunch of little rectangles
		- ![[Pasted image 20240215193223.png]]
	- Learning for Decision Trees
		- Many possible trees 
			- Cannot just try every tree and see what works, we need to target it more than that
		- Balance generalization and memorization
			- Avoid learning trees that are unnecessarily deep
			- We want the shallowest possible tree that still describes the data well
		- Learned function derived from an algorithm
			- Instead of learning by optimization, we are going to describe an algo that 