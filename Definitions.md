
| **Word** | **Definition** | Link |
| ---- | ---- | ---- |
| Ngram | Sequence of n words that we treat as a unit | [[Module 1 - Introduction & Framing#Machine Learning and Artificial Intelligence]] |
| Machine Learning | Algorithms improve through experience with data, distinct for its adaptability | [[Module 1 - Introduction & Framing#Machine Learning]] |
| Artificial Intellegence | Encompasses any technique enabling machines to mimic human behavior, broader than machine learning. | [[Module 1 - Introduction & Framing#Artificial Intelligence]] |
| Deep Learning | Specialized in processing large, complex datasets using multi-layered neural networks, a deeper subset of machine learning. | [[Module 1 - Introduction & Framing#Deep Learning]] |
| Supervised Learning | Relies on labeled datasets to train algorithms, making it specific for prediction based on known examples. | [[Module 1 - Introduction & Framing#Supervision]] |
| Unsupervised Learning | Identifies patterns or structures in unlabeled data, unique for its ability to find hidden insights without guidance. | [[Module 1 - Introduction & Framing#Supervision]] |
| Reinforcement Learning | Involves learning to make sequences of decisions by trial and error, distinct for its focus on sequential decision-making and reward-based | [[Module 1 - Introduction & Framing#Supervision]] |
| Function | Takes some input (x) applies a transformation f(x) and creates an output y | [[Module 1 - Introduction & Framing#What is a function?]] |
| Generalization | Capability to learn from some examples in a way that transfers to new examples | [[Module 1 - Introduction & Framing#Generalization]] |
| Ocram's Razor | All things being equal, the simpler explanation is better | [[Module 1 - Introduction & Framing#Generalization]] |
| Overfitting | The model has learned to fit the training model too well, at the expense of generalization | [[Module 1 - Introduction & Framing#Generalization]] |
| Calibration | Adjusting a model's output to ensure its predicted probabilities accurately reflect the true likelihood of an event or outcome | [[Module 1 - Introduction & Framing#Example 1 Lung Cancer Screening]] |
| Epoch | A single pass through the data |  |
| Batch |  |  |
| Fold |  |  |
| Training data |  |  |
| Loss curve |  | [[Module 2 - Linear Regression & Gradient Descent#The Loss Function]] |
| Gradient Descent |  |  |
| Learning Rate |  | [[Module 2 - Linear Regression & Gradient Descent#Learning Rate]] |
| Convergence |  | [[Module 2 - Linear Regression & Gradient Descent#Gradient Decent Intuition]] |
| Divergence |  |  |
| Hyper-parameter |  |  |
| Stochastic Gradient Descent |  | [[Module 2 - Linear Regression & Gradient Descent#Gradient Descent In Practice]] |
| Non-Convex |  | [[Module 2 - Linear Regression & Gradient Descent#Differentiable Loss Functions]] |
| Model |  |  |
| Loss |  |  |
| Parameters |  |  |
| Objective |  |  |
| Feature Vector |  |  |
| Z-score scaling |  |  |
| Bucketing |  |  |
| Log scaling |  |  |
| One-hot vectors |  |  |
| Multi-hot vector |  |  |
| Dense Representation |  |  |
| Sparse Representation |  |  |
| Sigmoid activation |  |  |
