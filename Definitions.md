
| **Word** | **Definition** | Link |
| ---- | ---- | ---- |
| Ngram | Sequence of n words that we treat as a unit | [[Module 1 - Introduction & Framing#Machine Learning and Artificial Intelligence]] |
| Machine Learning | Algorithms improve through experience with data, distinct for its adaptability | [[Module 1 - Introduction & Framing#Machine Learning]] |
| Artificial Intellegence | Encompasses any technique enabling machines to mimic human behavior, broader than machine learning. | [[Module 1 - Introduction & Framing#Artificial Intelligence]] |
| Deep Learning | Specialized in processing large, complex datasets using multi-layered neural networks, a deeper subset of machine learning. | [[Module 1 - Introduction & Framing#Deep Learning]] |
| Supervised Learning | Relies on labeled datasets to train algorithms, making it specific for prediction based on known examples. | [[Module 1 - Introduction & Framing#Supervision]] |
| Unsupervised Learning | Identifies patterns or structures in unlabeled data, unique for its ability to find hidden insights without guidance. | [[Module 1 - Introduction & Framing#Supervision]] |
| Reinforcement Learning | Involves learning to make sequences of decisions by trial and error, distinct for its focus on sequential decision-making and reward-based | [[Module 1 - Introduction & Framing#Supervision]] |
| Function | Takes some input (x) applies a transformation f(x) and creates an output y | [[Module 1 - Introduction & Framing#What is a function?]] |
| Generalization | Capability to learn from some examples in a way that transfers to new examples | [[Module 1 - Introduction & Framing#Generalization]] |
| Ocram's Razor | All things being equal, the simpler explanation is better | [[Module 1 - Introduction & Framing#Generalization]] |
| Overfitting | The model has learned to fit the training model too well, at the expense of generalization | [[Module 1 - Introduction & Framing#Generalization]] |
| Calibration | Adjusting a model's output to ensure its predicted probabilities accurately reflect the true likelihood of an event or outcome | [[Module 1 - Introduction & Framing#Example 1 Lung Cancer Screening]] |
| Epoch | A single pass through the data |  |
| Batch | A set of data points processed together in a machine learning algorithm.  |  |
| Fold | A subset used in cross-validation to validate a model during training. |  |
| Training data | Data used to train a machine learning model. |  |
| Loss curve | A graph showing the model's loss over time during training. | [[Module 2 - Linear Regression & Gradient Descent#The Loss Function]] |
| Gradient Descent | An optimization algorithm to minimize the loss function in learning. |  |
| Learning Rate | A parameter that determines the step size during gradient descent. | [[Module 2 - Linear Regression & Gradient Descent#Learning Rate]] |
| Convergence | The process of an algorithm approaching a stable solution. | [[Module 2 - Linear Regression & Gradient Descent#Gradient Decent Intuition]] |
| Divergence | When an algorithm moves away from a solution, often due to high learning rate. |  |
| Hyper-parameter | A parameter set prior to the learning process and not learned from data. |  |
| Stochastic Gradient Descent | A variant of gradient descent using a single data point at each iteration. | [[Module 2 - Linear Regression & Gradient Descent#Gradient Descent In Practice]] |
| Non-Convex | A function with multiple local minima and maxima. | [[Module 2 - Linear Regression & Gradient Descent#Differentiable Loss Functions]] |
| Model | An abstract representation learned from data to make predictions. |  |
| Loss | A measure of how far a model's predictions are from the actual values. | [[Module 2 - Linear Regression & Gradient Descent#The Loss Function]] |
| Parameters | Variables in a model that are learned from the training data. |  |
| Objective | The goal or function that a machine learning model is trying to optimize. |  |
| Feature Vector | An array of numerical features representing an instance in data. |  |
| Z-score scaling | Standardization of data by subtracting mean and dividing by standard deviation. |  |
| Bucketing | Grouping continuous variables into discrete categories. |  |
| Log scaling | Applying logarithmic transformation to scale data. | [[Module 3 - Feature Engineering#Feature Engineering]] |
| One-hot vectors | Binary vectors representing categorical data, with exactly one high bit. |  |
| Multi-hot vector | Binary vectors representing categorical data, with multiple high bits. |  |
| Dense Representation | A data representation where most elements are non-zero. |  |
| Sparse Representation | A data representation mostly made up of zeros. |  |
| Sigmoid activation | An S-shaped function mapping inputs to values between 0 and 1. |  |
